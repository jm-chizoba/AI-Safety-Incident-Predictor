{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6501fc1f",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "This notebook evaluates the AI-Safety-Incident-Predictor model on test data, including confusion matrix and key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf45eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed22ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load synthetic dataset (or replace with your dataset)\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'shift_hours': np.random.randint(6, 13, 100),\n",
    "    'weather': np.random.choice(['Sunny', 'Rainy', 'Cloudy'], 100),\n",
    "    'equipment_used': np.random.choice(['Standard', 'Heavy', 'Light'], 100),\n",
    "    'past_incidents': np.random.randint(0, 5, 100),\n",
    "    'safety_training_hours': np.random.randint(1, 10, 100),\n",
    "    'incident_occurred': np.random.choice([0, 1], 100, p=[0.7, 0.3])\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df_encoded = pd.get_dummies(df, columns=['weather', 'equipment_used'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e12d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_encoded.drop('incident_occurred', axis=1)\n",
    "y = df_encoded['incident_occurred']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba050254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Incident', 'Incident'], yticklabels=['No Incident', 'Incident'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
